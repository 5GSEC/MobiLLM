from .tools.sdl_apis import *
from .tools.mitre_apis import *
from .tools.control_apis import *
# Import necessary libraries from LangChain and Google
# Make sure you have them installed:
# pip install langchain langchain_google_genai google-generativeai
import os
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.agents import tool, AgentExecutor, create_react_agent
from langchain_core.prompts import PromptTemplate # For custom prompts if needed
from langchain.tools import Tool # Import the Tool class
from langchain import hub # For pre-built prompts
from langchain.memory import ConversationBufferWindowMemory # Import memory

# --- Default Task Backgrounds (can be overridden in __init__) ---
DEFAULT_CHAT_TASK_BACKGROUND = """
You are a highly specialized 5G network analysis assistant. Your primary role is to help network operators understand network behavior, diagnose issues, manange network services, and interpret telemetry data like MobiFlow. You should focus on data retrieval, explanation, and root cause identification. You have access to tools for querying network data and events, as well as orchestrating network services.
"""

DEFAULT_SECURITY_TASK_BACKGROUND = """
You are a 5G cybersecurity response assistant. Your mission is to help network operators react to identified threats and anomalies. This includes explaining security events (potentially generated by MobieXpert or MobiWatch xApps), proposing countermeasures, and orchestrating defensive actions using the provided tools. Focus on security analysis and mitigations.
"""

# --- Base ReAct Prompt Template String (TASK_BACKGROUND will be formatted in) ---
BASE_REACT_PROMPT_TEMPLATE_STR = """
{TASK_BACKGROUND}

You have access to the following tools:
{{tools}}

Use the following format for your reasoning process:

Question: the input question you must answer
Thought: you should always think about what to do to answer the question based on the task, available tools, and conversation history.
Action: the action to take, should be one of [{{tool_names}}]
Action Input: the input to the action. If the tool's description does not state it requires an argument, simply invoke the tool without argument by providing an empty string or "N/A". Otherwise, provide the appropriate input.
Observation: the result of the action

... (this Thought/Action/Action Input/Observation can repeat N times)

Thought: I now have enough information to answer the user's question.
Final Answer: the final answer to the original input question.

Previous conversation history:
{{chat_history}}

Begin!

Question: {{input}}
Thought:{{agent_scratchpad}}
"""

class MobiLLMAgent:

    def __init__(self, google_api_key: str=None, gemini_llm_model: str="gemini-2.5-flash-preview-04-17"):
        self.init_completed = False
        self.gemini_llm_model = gemini_llm_model
        # --- Configuration ---
        # IMPORTANT: Set your GOOGLE_API_KEY as an environment variable.
        # LangChain's Google Generative AI integration will automatically pick it up.
        # If you don't use dotenv, ensure the environment variable is set in your system.
        # Alternatively, you can pass api_key directly to ChatGoogleGenerativeAI, but env var is preferred.
        if not os.getenv("GOOGLE_API_KEY") and google_api_key == None:
            print("Warning: GOOGLE_API_KEY not found in environment variables.")
            print("Please set it for the LangChain Gemini LLM to work.")
            return
        elif google_api_key is not None:
            os.environ["GOOGLE_API_KEY"] = google_api_key
            # You could set it here as a fallback, but it's not recommended for production:
            # os.environ["GOOGLE_API_KEY"] = "YOUR_ACTUAL_API_KEY"

        # --- LLM Initialization ---
        # Initialize the Gemini LLM through LangChain
        try:
            self.llm = ChatGoogleGenerativeAI(model=self.gemini_llm_model, temperature=0.3)
            # You can adjust temperature and other parameters as needed.
            # temperature=0 makes the model more deterministic, higher values make it more creative.
        except Exception as e:
            print(f"Error initializing Gemini LLM: {e}")
            print("Ensure your GOOGLE_API_KEY is set correctly and you have internet access.")
            return

        # --- Tool Definitions ---
        # Your custom tools need to be wrapped in the LangChain Tool class.
        # The 'name' should be a string the LLM can use to call the tool.
        # The 'description' is crucial; it's taken from the function's docstring if not provided explicitly.
        # Ensure your functions in sdl_apis.py have clear and informative docstrings.

        # Example of how to get docstrings (if needed, but Tool class does this automatically)
        # desc_mobiflow_all = get_ue_mobiflow_data_all.__doc__
        # desc_mobiflow_desc = get_ue_mobiflow_description.__doc__
        # desc_sdl_event_data = fetch_sdl_event_data_osc.__doc__
        # desc_event_desc = get_event_description.__doc__

        try:
            # Define a list of tools the agent can use
            # Tool set for analysis and orchestration
            self.mobillm_chat_tools = [
                get_ue_mobiflow_data_all_tool,
                get_ue_mobiflow_data_by_index_tool,
                get_ue_mobiflow_description_tool,
                get_bs_mobiflow_data_all_tool,
                get_bs_mobiflow_data_by_index_tool,
                get_bs_mobiflow_description_tool,
                fetch_sdl_event_data_all_tool,
                fetch_sdl_event_data_by_ue_id_tool,
                fetch_sdl_event_data_by_cell_id_tool,
                get_event_description_tool,
                fetch_service_status_tool,
                build_xapp_tool,
                deploy_xapp_tool,
                unDeploy_xapp_tool,
            ]

            # Tool set for security threat response
            self.mobillm_security_tools = [
                get_ue_mobiflow_data_all_tool,
                get_ue_mobiflow_data_by_index_tool,
                get_ue_mobiflow_description_tool,
                fetch_sdl_event_data_all_tool,
                fetch_sdl_event_data_by_ue_id_tool,
                fetch_sdl_event_data_by_cell_id_tool,
                get_event_description_tool,
                get_all_mitre_fight_techniques,
                get_mitre_fight_technique_by_id,
                get_ran_cu_config_tool,
                update_ran_cu_config_tool,
                # reboot_ran_cu_tool
            ]
        except AttributeError as e:
            print(f"Error creating Tools: {e}. This often means a function is missing a docstring or is not correctly imported.")
            print("Ensure all functions from sdl_apis.py (get_ue_mobiflow_data_all, get_ue_mobiflow_description, fetch_sdl_event_data_osc, get_event_description) are defined, imported, and have docstrings.")
            return
        except Exception as e:
            print(f"An unexpected error occurred while defining tools: {e}")
            return

        # --- Memory Initialization ---
        # k is the number of past interactions to remember
        # memory_key is the variable name in the prompt that will contain the chat history
        # return_messages=True ensures the history is formatted as a list of messages, suitable for chat models
        memory_key = "chat_history"
        self.memory = ConversationBufferWindowMemory(
            memory_key=memory_key,
            k=5, # Remember the last 5 interactions
            return_messages=True
        )

        # --- Agent Creation ---
        # LangChain provides helper functions to create agents.
        # For ReAct, we can use `create_react_agent`.
        # It requires an LLM, the tools, and a prompt.

        # --- MobiLLM Chat Agent Setup ---
        chat_prompt_str = BASE_REACT_PROMPT_TEMPLATE_STR.format(TASK_BACKGROUND=DEFAULT_CHAT_TASK_BACKGROUND)
        try:
            self.chat_prompt = PromptTemplate(
                input_variables=["chat_history", "input", "agent_scratchpad", "tools", "tool_names"],
                template=chat_prompt_str
            )
            self.chat_agent = create_react_agent(self.llm, self.mobillm_chat_tools, self.chat_prompt)
            self.chat_agent_executor = AgentExecutor(
                agent=self.chat_agent,
                tools=self.mobillm_chat_tools,
                memory=self.memory, # Shared memory
                verbose=True,
                handle_parsing_errors=True,
                max_iterations=10
            )
        except Exception as e:
            print(f"Error creating MobiLLM Chat Agent: {e}")
            return

        # --- MobiLLM Security Agent Setup ---
        security_prompt_str = BASE_REACT_PROMPT_TEMPLATE_STR.format(TASK_BACKGROUND=DEFAULT_SECURITY_TASK_BACKGROUND)
        try:
            self.security_prompt = PromptTemplate(
                input_variables=["chat_history", "input", "agent_scratchpad", "tools", "tool_names"],
                template=security_prompt_str
            )
            self.security_agent = create_react_agent(self.llm, self.mobillm_security_tools, self.security_prompt)
            self.security_agent_executor = AgentExecutor(
                agent=self.security_agent,
                tools=self.mobillm_security_tools,
                memory=self.memory, # Shared memory
                verbose=True,
                handle_parsing_errors=True,
                max_iterations=10
            )
        except Exception as e:
            print(f"Error creating MobiLLM Security Agent: {e}")
            return

        self.init_completed = True
    
    def init_successful(self) -> bool:
        return self.init_completed

    def chat(self, user_query: str) -> dict:
        if not self.init_successful():
            return {"output": "Agent initialization failed. Please check the logs for errors."}
        try:
            # The 'input' key in the dictionary corresponds to the {input} placeholder in the prompt.
            # The 'agent_scratchpad' is handled internally by the AgentExecutor.
            # For chat-based prompts, the input might be structured differently,
            # e.g., {"input": user_query, "chat_history": []}
            # The specific input keys depend on the prompt template used.
            # The `hwchase17/react-chat` prompt expects "input".
            return self.chat_agent_executor.invoke({"input": user_query})
        except Exception as e:
            return {"output": f"Error during agent execution for query '{user_query}': {e}"}
            # This can happen due to LLM errors, tool errors, or parsing errors
            # if handle_parsing_errors doesn't catch everything.

    def security_analysis(self, user_query: str) -> dict:
        if not self.init_successful():
            return {"output": "Agent initialization failed. Please check the logs for errors."}
        try:
            # The 'input' key in the dictionary corresponds to the {input} placeholder in the prompt.
            # The 'agent_scratchpad' is handled internally by the AgentExecutor.
            # For chat-based prompts, the input might be structured differently,
            # e.g., {"input": user_query, "chat_history": []}
            # The specific input keys depend on the prompt template used.
            # The `hwchase17/react-chat` prompt expects "input".
            return self.security_agent_executor.invoke({"input": user_query})
        except Exception as e:
            return {"output": f"Error during agent execution for query '{user_query}': {e}"}
            # This can happen due to LLM errors, tool errors, or parsing errors
            # if handle_parsing_errors doesn't catch everything. 

# --- Running the Agent ---
if __name__ == "__main__":

    queries = [
        # "How many UEs are currently connected to cell ID 12345678?"

        # "How many Base stations are currently connected to the network?"

        "How many UEs are currently connected to base station ID 10000?"

        # "Explain the anomaly events detected on UE ID 38940, analyze the UE traffic data and provide a more in-depth analysis beyond the provided descriptions in the event data, but keep the response as concise as possible and up to the point."

        # "Explain the Blind DoS attack detected and propose mitigation of this attack"

        # "What is the status of the E2 manager?"

        # '''
        # You are a cybersecurity expert focused on 5G network security. 
        # Analyze the Blind DoS detected as an attack event in the network.
        # Provide the following information. 
        # Keep the response as concise as possible and up to the point. Produce the output in well-formatted plain-text.
        # 1. An in-depth explanation of the threat or anomaly beyond the description, by analyzing the associated MobiFlow data.
        # 2. Recommended effective countermeasures to address this problem.
        # '''

        # '''
        # You are a cybersecurity expert focused on 5G network security. 
        # Analyze the MobiWath generated events detected on UE ID 38940 in the network.
        # Provide the following information. 
        # Keep the response as concise as possible and up to the point. Produce the output in well-formatted plain-text.
        # 1. An in-depth explanation of the threat or anomaly beyond the description, combine the analysis using the event data and associated MobiFlow data if necessary.
        # 2. Recommended effective countermeasures to address this problem.
        # '''

        # '''
        # Provide an in-depth analysis on the events detected on UE 38940, including:
        # 1. An explanation of the threat or anomaly beyond the given description, combine the analysis using the event data and associated MobiFlow data of the UE.
        # 2. Based on the analysis report, try to classify the identified threats using the MiTRE fight techniques. For the output, please provide the MiTRE Fight technique ID (such as "FGT1588") that you believe the threat or anomaly belongs to.
        # 3. If you have classified the threat or anomaly into a specific MiTRE Fight technique, report the corresponding mitigations in that MiTRE Fight technique.
        # '''
    ]

    for user_query in queries:
        print(f"\n\nExecuting query: \"{user_query}\"")
        print("================================================")
        mobillm_agent = MobiLLMAgent()
        if mobillm_agent.init_successful() is True:
            response = mobillm_agent.chat(user_query)
            # response = mobillm_agent.security_analysis(user_query)
            print("------------------------------------------------")
            print(f"Final Answer from Agent: {response['output']}")
            print("================================================")
        print("\n")

